# 2. アプリケーションの概要

このラボでは、Azure 上でシンプルながら強力なイベント駆動アプリケーションを構築・デプロイします。
シナリオは複数のIoT センサーがランダムな間隔で温度データを送信するというものです。
バックエンドのサマライザーが **Azure Cosmos DB の Change Feed** を利用してリアルタイムにデータを処理し、ビジュアライザーが集約された結果を Web インターフェイスに表示します。

この例は IoT を題材にしていますが、アーキテクチャは様々な分野で応用できます。例えば:

* **ゲームのランキングシステム** – プレイヤーのスコアを更新し、全件を再処理することなく順位を計算します。
* **EC サイト** – 注文や在庫の変更に応じておすすめ商品を更新したり、ワークフローを起動します。
* **金融システム** – 取引イベントを分析パイプラインに送り、不正検出や監査に利用します。

### Change Feed を利用する理由

Change Feed は、データベース全体をポーリングすることなく新しいデータに反応できる軽量な仕組みです。
コンテナに対する各挿入・更新が順序通りに一回だけ記録されるため、アプリケーションは、このストリームを処理するだけで最新の状態を把握できます。

Change Feed の利点は次の通りです:

* **リアルタイム処理** – 書き込み直後にデータがフィードに現れるため、ライブダッシュボードや通知に最適です。
* **スケーラビリティ** – フィードはコンテナのパーティションに従って分割されるので、複数のコンシューマーが並列に処理できます。
* **高可用性** – Cosmos DB は読み書きに 99.999 % の可用性を保証し、Change Feed は **少なくとも 1 回** の配信保証を提供します。
* **イベントソーシング** – すべてのイベント履歴を再生して状態やマテリアライズドビューを再構築できます。

このラボを通じて、Change Feed がイベント駆動パイプラインの構築をどれほど簡素化するかを体験してください。

## シナリオをさらに深掘りする

一見するとこのアプリケーションはシンプルな IoT デモですが、このパターンは実際の業務にも適用できます。
ジェネレータの各センサーはイベントのストリームを表しています。
製造現場では温度や振動、圧力センサーがデータを送信し、物流では荷物が GPS 座標やステータスを報告し、ゲームではプレイヤーの操作がスコアリングイベントになります。

従来のシステムでは、新しいイベントを取得するために、データベースやメッセージキューを定期的にポーリングする必要がありました。
Cosmos DB の Change Feed を使用すると、変更を記録するストリームそのものを購読できます。
サマライザーは各センサーパーティションごとに移動窓で統計（最大・最小・平均）を計算するストリーミング集計器として機能します。
これはストリーミング分析で一般的な **スライディングウィンドウ集計** であり、ロジックを拡張してパーセンタイルや標準偏差、異常検知などを計算することも簡単です。

ビジュアライザーはデータをユーザーに表示する役割を担います。
各チャートは独立しており、必要に応じて更新されます。
サマライザーが集計結果を Cosmos DB に書き戻すため、ビジュアライザーは Change Feed を直接処理することなく単純なクエリで最新状態を取得できます。
生データを可視化したい場合は `readings` コンテナからデータを取得したり、Azure Data Explorer などの時系列データベースに取り込むことができます。

### アーキテクチャの拡張

ここで示したパターン **「プロデューサ → Change Feed → コンシューマ → マテリアライズドビュー」** は多くのイベント駆動アプリの基本です。
より複雑なアプリケーションでは、サマライザーの後段に追加のマイクロサービスを配置できます。例えば:

* **通知サービス** – 温度が閾値を超えたときにメールや SMS を送るアラートサービスを追加します。
* **機械学習推論** – 最近のセンサーデータを訓練済みモデルに入力し、故障予測やメンテナンス予測を行います。
* **データレイクへの連携** – Change Feed のイベントを Azure Data Lake Storage に書き出し、オフライン分析や機械学習の学習用データとして利用します。
  Azure Fabric の Mirroring を使用するとほぼリアルタイムでデータを複製できます。

Container Apps とマネージド ID のモジュール性により、データベースを変更することなくこれらのサービスを追加できます。
各コンシューマは Change Feed を読み取り、独自のロジックに従ってイベントを処理します。
このデカップリングにより、チームは迅速に機能を追加し、独立してスケールアウトできます。

## ストリーミング分析パターンの理解

Change Feed パターンを真に理解するには、アプリケーションを **ストリーム処理** の視点で捉えることが重要です。
ストリーミングシステムには 2 つの時間概念があります。
**イベント時刻**（センサーがデータを生成した時刻）と **処理時刻**（アプリケーションがイベントを受け取った時刻）です。
サマライザーのような集約器はイベント時刻順に移動ウィンドウを形成し、その中で統計を計算します。
このラボでは各センサーにつき最新 10 件のイベントをウィンドウとしていますが、本番環境では 5 分間など時間単位の窓や、一定のアイドル時間で閉じるセッションウィンドウを用いることもあります。

ウィンドウの種類によって結果や実装の複雑さが異なります。
Apache Flink や Azure Stream Analytics のようなライブラリは複雑なウィンドウ演算を提供しますが、Cosmos DB の Change Feed と独自ロジックで多くのパターンに対応できます。
例えば、「直近 1 時間の平均値」を計算するには、`utc_now() - timedelta(hours=1)` より新しい読み取りを `readings` コンテナから取得し、平均値を計算するだけです。
パーティションキーを `/sensor_id` にしているため、これらのクエリは効率的に実行できます。

サマライザーが採用しているのは典型的な **スライディングウィンドウ集計** です。
新しい読み取りが到着するたびに、ウィンドウから最も古いデータを除外し、統計を再計算します。
このパターンはより複雑な指標へも拡張できます。
例えば、人口標準偏差を計算するために `statistics.pstdev` を使用したり、中央値を求めるためにヒープを二つ用いてバランス木を構築することも可能です。
10 件以上のデータを保持する場合は、ウィンドウ用の別コンテナを用意したり、Redis のようなキャッシュを活用して `readings` コンテナへの読み取りを減らすのがよいでしょう。

### イベント駆動マイクロサービスアーキテクチャ

このラボは **プロデューサ–コンシューマ** アーキテクチャの典型例です。
ジェネレータとサマライザーは Change Feed を通じて疎結合になっているため、以下のメリットが得られます。

* **回復性** – サマライザーが停止しても、ジェネレータの書き込みは継続され、Change Feed にイベントが蓄積されます。
  サマライザーが再開すると途中から処理を再開できます。
* **スケーラビリティ** – ジェネレータを数千インスタンスに増やしてもサマライザーを変更する必要はありません。
  処理量が増えたらサマライザーを水平スケールして対応できます。
* **拡張性** – 既存のプロデューサを変更することなく、通知サービスや機械学習サービスなど新しいコンシューマを Change Feed に接続できます。

大規模なシステムでは、Change Feed のイベントを Event Hubs や Kafka などのメッセージブローカーに流し込むこともあります。
これによりバッファリングや順序保証が加わり、**ファンアウト**（1 件のイベントを複数のコンシューマに配信）や **ファンイン**（複数のプロデューサが同じストリームに書き込む）のパターンを実現できます。
Change Feed と Azure Functions や Logic Apps を組み合わせて多段ワークフローをオーケストレーションすることも可能です。

### 高度な拡張例

この基本アーキテクチャはさまざまな方向へ発展できます。

* **グラフ検索やベクトル検索** – IoT ではセンサーやデバイスをノードとして表し、グラフ探索で異常パターンを検出したり、製品間の関係性を分析することが可能です。
  また、[postgres‑agentic‑shop](https://github.com/Azure-Samples/postgres-agentic-shop) では AI エージェントが Postgres データベースに対してベクトル検索やグラフクエリを実行する例が紹介されています。
  センサーデータに埋め込みベクトルを付与し、類似パターンを検索する応用も考えられます。
* **マテリアライズドビュー** – 集計を都度計算するのではなく、時間単位でまとめたビューを別コンテナに保持することができます。
  サマライザーを拡張し、1 時間ごと・1 日ごとの統計を保存すれば、ダッシュボードはクエリだけで結果を参照できます。
* **イベントソーシングと監査ログ** – Change Feed はすべての変更を永続的に保持するため、イベントソーシングパターンに適しています。
  新しい集計ロジックを導入する際に、過去のイベントを最初から再生することで状態を再構築できます。
* **機械学習との統合** – センサーのストリームを異常検知や予測モデルに入力し、閾値を超えた際に Azure Functions で通知を送るといった拡張も容易です。

ストリーム処理やマイクロサービスアーキテクチャの概念を理解すると、このデモを自分のユースケースに合わせて柔軟に応用できるようになります。

## 実際のユースケース

ここで扱うパターンは温度センサーの例に留まりません。
Change Feed を活用したアーキテクチャが活躍する具体的なシナリオをいくつか紹介します。

* **車両管理や物流** – トラックやコンテナが定期的に GPS 座標や積載状況を送信し、Change Feed プロセッサが地図ダッシュボードを更新したり、温度逸脱のアラートを発行します。
* **スマートビルディング** – ビル内のセンサーから人感、CO₂ 濃度、電力消費量などを受信し、リアルタイムに空調や照明を制御します。
  デジタルツインへの連携や予防保全にも応用できます。
* **ゲームの状態同期** – マルチプレイヤーゲームではランキングやインベントリの更新を低遅延で共有する必要があります。
  Cosmos DB にイベントを書き込み、Change Feed からマテリアライズドビューを生成すれば、リーダーボードを再計算することなく最新状態を取得できます。
* **金融取引** – 決済ゲートウェイやトレーディングシステムは注文・約定のストリームを生成します。
  Change Feed コンシューマはリアルタイムのリスクチェックや口座残高の更新を行い、不正検知パイプラインへデータを流し込むことができます。

これらの例から、プロデューサ・Change Feed・コンシューマ・可視化という基本要素を組み替えることで、多様な領域に応用できることが分かります。
イベントの性質（ボリューム、遅延許容、順序）を把握し、適切な処理方式とストレージパターンを選択しましょう。

## 設計上の選択肢とトレードオフ

本番環境でこのアーキテクチャを採用する際には、いくつかの設計判断が求められます。代表的な項目を挙げます。

* **スループットモード (Serverless / プロビジョンド)** – Serverless はコスト管理が容易ですが、コンテナサイズや同時実行数に制限があります。
  Provisioned は RU/s を確保することで安定したレイテンシを得られます。
  Autoscale はその中間で、ピークに合わせて自動調整します。
* **整合性レベル** – Cosmos DB には `Strong` から `Eventual` まで 5 種類の整合性モデルがあります。
  強い整合性はマルチリージョンでレイテンシが大きくなりますが厳格な順序保証が必要な場合に選びます。
  分析用途では `Bounded Staleness` や `Session` で十分なことが多いです。
* **イベントの粒度** – センサーごとに 1 ドキュメントを書くか、複数イベントをまとめて 1 ドキュメントにするか。
  細かい粒度は柔軟に処理できますが、書き込み回数が増えます。
  高スループット用途ではバッチ化してダウンサイド処理を導入する設計も検討します。
* **集計ロジック** – このデモでは最大値・最小値・平均を扱っていますが、実際には中央値やパーセンタイル、加重平均など複雑な統計が必要になることもあります。
  ウィンドウサイズ（時間ベースか件数ベースか）によって応答性と精度が変わるため、ドメイン要件に合わせて調整します。
* **状態管理** – サマライザーでは最近 9 件をメモリに保持しています。
  ウィンドウが大きくなる場合や水平スケールする場合は、`sensorHistory` といった専用コンテナや、Redis キャッシュに状態を保持することで複数インスタンスから参照可能にできます。

これらの選択を文書化し、Azure の Well‑Architected Framework の観点（コスト最適化、性能効率、信頼性）で評価すると、スケーラブルで安定したシステムが構築できます。

## デモの発展的な拡張

基礎を理解した後は、以下のような方法でデモを拡張してみましょう。

* **センサーやデータソースを増やす** – ジェネレータを拡張して数百台のデバイスをシミュレートしたり、MQTT や OPC‑UA ゲートウェイ経由で実際のハードウェアからデータを取得します。
* **追加の統計を計算する** – `collections.deque` でソート済みスライディングウィンドウを保持し、中央値やパーセンタイルを効率的に算出するなど、複雑な統計値を導入します。
* **状態を別のデータストアに保持** – 9 件の履歴をメモリだけでなく、専用の `sensorHistory` コンテナや Azure Cache for Redis に保存することで、複数のサマライザーが同じ状態を共有し、再起動後も続きから処理できます。
* **API やストリームを公開する** – 集計結果を REST や gRPC で提供したり、Event Hubs や Service Bus に公開して他のマイクロサービスが購読できるようにします。
* **認証と権限管理** – Visualizer のエンドポイントを保護するために Azure App Service Authentication や API Management を利用し、コンテナレベルでロールを割り当てるなど細かなアクセス制御を実現します。

こうした拡張によって、このラボは実運用にも耐えるリファレンスアーキテクチャへと成長します。
次章以降では、各コンポーネントの実装をさらに詳しく解説します。

## 発展的なイベント駆動パターン

このラボで紹介した基本的なパイプラインに加えて、さまざまなイベント駆動アーキテクチャを組み合わせることができます。

* **ファンアウト／ファンインパイプライン** – サマライザーを複数のコンシューマーに分割し、Event Hubs や Service Bus トピック経由でデータを分配します。
  例えば、1 つは統計計算に、もう 1 つは閾値アラートに使用し、最後に結果を集約します。
* **CQRS とコマンド／クエリの分離** – 書き込みと読み取りを分離してスケールさせる設計です。
  サマライザーは Change Feed からイベントを取得し、読み取り専用のデータモデルにイベントを発行します。
  これにより書き込みのレイテンシと読み取りのスケーラビリティを分離できます。
* **タイムトラベルとリプレイ** – Change Feed は追記型ログなので、継続トークンを巻き戻すことで過去のイベントを再生できます。
  これによりロジック変更時に派生状態を再構築したり、新しい統計をバックフィルすることが可能です。
* **AI や検索との統合** – Cosmos DB の `vector` を用いてベクトル埋め込みやグラフデータを保存し、異常検知や類似パターン検索を行うこともできます。
  Azure Cognitive Search や Azure OpenAI API と組み合わせれば、センサーの読み取り履歴からパターンを見つけ出すことができます。
* **エッジからクラウドへの連携** – IoT Edge モジュールで前処理や異常検知を行い、その結果をクラウドへ送って Change Feed パイプラインに取り込みます。
  エッジとクラウドで同じ設計パターンを再利用できます。

## マルチテナント・マルチセンサー対応

実際のシステムでは数百〜数千のセンサーが複数の顧客に属している場合があり、このアーキテクチャを適用する際には以下の点を考慮する必要があります。

* **パーティション戦略** – `tenant_id` と `sensor_id` を組み合わせた合成パーティションキー（例: `"tenant1-sensor42"`）で均等に分散させます。
  ホットパーティションを避けるためにランダムサフィックスやハッシュ ID を利用することも検討します。
* **名前空間分離** – テナントごとに Cosmos DB のデータベースやコンテナを分けることでデータ隔離を実現します。
  代わりに Azure Entra ロールとポリシーでアクセス制御を実装する方法もあります。
* **同時実行制御** – 複数のサマライザーインスタンスで同じセンサーのストリームを処理する場合は、冪等な書き込みを設計し、Change Feed Processor ライブラリのリース機構を利用して重複処理を防ぎます。
* **時間的意味** – スライディングウィンドウをイベント時刻ベースにするか、処理時刻ベースにするか決めます。
  イベント時刻ベースでは遅延到着や順序の前後を処理するウォーターマークが必要になりますが、実世界の時間軸と一致します。
  処理時刻ベースは簡便ですが実際のタイムラインとずれる可能性があります。

分散システムでよく語られる **Kappa アーキテクチャ** などの概念を参考にしながら、これらの設計を検討することで、ラボを超えたスケールへ拡張する際の指針になります。
