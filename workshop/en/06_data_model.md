# 6. Database and Data Model

This demo uses a single Azure Cosmos DB database named **`sensors`** with three
containers.  Understanding the purpose of each container and its
partitioning strategy is key to designing scalable change feed solutions.

## `readings` Container

The `readings` container stores the raw temperature measurements emitted by
the generator.  Each document has the following structure:

```json
{
  "id": "<uuid>",
  "sensor_id": "sensor-1",
  "temperature": 27.5,
  "timestamp": "2025-10-29T01:05:58Z"
}
```

* **Partition key** – `/sensor_id`.  This ensures that all readings for a
  given sensor are stored in the same logical partition, which improves
  change feed processing and allows efficient queries for recent readings.
* **TTL** – not enabled in this demo.  In a production scenario you could
  enable Time To Live on the `readings` container to automatically purge
  old data and control storage costs.

When deciding on throughput for the `readings` container, consider the
expected ingestion rate.  Each insert consumes a small number of RU/s
(request units per second).  If you have five sensors emitting every few
seconds, 400 RU/s autoscale is more than sufficient.  For tens of
thousands of sensors, scale up accordingly.  You can monitor consumed
RU/s via Azure Monitor metrics and adjust the autoscale max RU/s in
`main.bicep` as needed.

Because change feed follows partition boundaries, using `/sensor_id` as
the partition key ensures that reading the last N events for a sensor
requires scanning a single partition.  If you instead partitioned on a
time bucket (e.g. `/day`), you would need to read from multiple
partitions for each sensor, complicating your queries.  Design your
schema around the operations you need to perform.【437749716107481†L47-L66】

## `summaries` Container

The `summaries` container holds aggregated statistics generated by the
summariser.  A summary document represents the max, min and average
temperature of the last 10 readings for a sensor:

```json
{
  "id": "<uuid>",
  "timestamp": "2025-10-29T01:06:10Z",
  "sensor_id": "sensor-1",
  "max_temp": 33.7,
  "min_temp": 21.4,
  "avg_temp": 28.5
}
```

* **Partition key** – `/sensor_id`, matching the `readings` container so
  that summarised data stays co‑located with its source readings.
* **Retention** – each summary represents a point in time.  You can
  implement TTL on this container to retain only recent summaries for
  visualisation.

Since summaries are tiny documents, you can afford to keep more history
than with raw readings.  However, if your summariser writes every time
a reading arrives (as in this lab), the number of summary documents
grows quickly.  Enabling a TTL of a few hours or days ensures that the
visualiser queries only recent data.  TTL is specified in seconds via
the container properties or the Azure Portal.  Note that deleting items
via TTL still generates change feed events, so adjust your consumer
logic if you listen to the `summaries` change feed.

## `leases` Container

The change feed processor uses a leases container to track progress.  A
lease document records the continuation token and other metadata needed to
resume reading from the change feed.  Each consumer must have its own
leases container or lease prefix to avoid interfering with other
processors.

The summariser’s lease ID is fixed in `summarizer.py` as
`readings_changefeed_lease`.

## Why Partition Keys Matter

Because the change feed follows the same partitioning as the source
container, choosing an appropriate partition key is critical.  Using
`/sensor_id` ensures that updates to a given sensor are processed in
order and allows the summariser to query the latest 10 readings by
scanning a single partition.

In multi‑tenant scenarios, you may choose a composite key that includes
tenant ID and sensor ID (e.g. `/tenant_id/sensor_id`).  This prevents
two tenants from sharing partitions and isolates workloads.  Cosmos DB
also supports **synthetic keys** if your natural key is not evenly
distributed.  By concatenating a hash prefix (e.g. `md5(sensor_id)`) to
the key, you can spread writes across multiple partitions while
preserving order within a single sensor.

Remember that once set, a container’s partition key cannot be changed
without migrating data.  Spend time up front evaluating your access
patterns, throughput requirements and distribution characteristics.

## Index and TTL Configuration

By default Cosmos DB indexes every property in your documents.  While
this provides maximum query flexibility, it consumes additional
storage and RU/s.  You can customise the index policy to balance
performance and cost:

* **Include only queried properties** – If certain properties (such
  as metadata blobs or arrays) are not used in queries or filters,
  exclude them from the index to reduce index size.  Define
  `excludedPaths` in the container properties or via Bicep.  Paths
  support wildcard patterns (e.g. `/payload/*`).
* **Composite indexes** – When your queries filter or sort on multiple
  fields, define a composite index to avoid cross‑partition scans.  The
  summariser queries on `sensor_id` and orders by `timestamp`, so a
  composite index on those two fields could speed up range queries.
* **TTL and soft delete** – You can enable a default Time To Live on
  containers so that items expire automatically after a certain
  number of seconds.  This is useful for `readings` and `summaries`
  containers where old data is not needed indefinitely.  TTL deletions
  generate change feed events, so ensure your consumers handle
  `expiry` events gracefully.

## Storage Patterns and Hot/Cold Data

Not all data needs to reside in Cosmos DB forever.  Consider a
multi‑tier storage strategy:

* **Hot tier (Cosmos DB)** – Recent sensor data and summaries that
  require low‑latency access and high availability remain in the
  `readings` and `summaries` containers.  TTL ensures that data older
  than a configured horizon (e.g. 24 hours) is removed automatically.
* **Cold tier (Blob or Data Lake)** – Periodically archive older
  readings to Azure Blob Storage or Data Lake using Azure Data
  Factory or Synapse pipelines.  This reduces Cosmos DB storage costs
  while preserving historical data for batch analytics.
* **Aggregated tier** – Pre‑compute aggregates at hourly or daily
  granularity and store them in a separate container or analytical
  store (e.g. Synapse Link).  The visualiser can query this tier for
  long‑term trends without scanning millions of raw readings.

Designing an appropriate data retention and archival policy improves
cost efficiency and keeps the change feed manageable.

## RU Monitoring and Performance Tuning

Cosmos DB charges for throughput and storage.  To optimise RU
consumption:

* **Monitor metrics** – Use Azure Monitor to track RU consumption,
  request latency, throttled requests and storage utilisation.  Set
  alerts when usage approaches the max RU/s to avoid throttling.
* **Adjust autoscale limits** – If the workload is bursty, set
  `maxThroughput` high enough to accommodate spikes.  For steady
  workloads, consider manual throughput with scheduled changes (using
  `az cosmosdb sql container throughput update` in a cron job).
* **Optimise queries** – Retrieve only necessary fields by projecting
  specific properties in your SQL (e.g. `SELECT c.sensor_id, c.temp`)
  rather than `SELECT *`.  Use parameterised queries to take
  advantage of server‑side cache.
* **Use buffered writes** – Group multiple writes in a single request
  where possible.  Batch operations (stored procedures or bulk
  executor library) reduce network overhead and improve throughput.

By combining careful partition key design, indexing strategies, and
monitoring, you can build a data layer that scales with your
applications.

## Advanced Modelling Patterns

When designing schemas for sensor telemetry, there are several
advanced patterns to consider:

* **Single vs multiple containers** – You can store readings for all
  sensors in a single container (partitioned by `sensor_id`), as
  this lab does.  Alternatively, create a dedicated container per
  sensor type or customer to isolate workloads and enforce SLAs.
  Use database-level throughput to share RU/s across containers.
* **Schema versioning** – As device firmware evolves, new fields may
  be added.  Use optional properties and a `_schemaVersion` field
  to track changes.  Avoid storing large binary payloads in the
  same document; instead reference blobs in Azure Storage.
* **Compound documents** – For certain workloads it is efficient to
  aggregate multiple events into a single document.  For example,
  store a day’s worth of readings in an array property to minimise
  RU consumption.  However, this makes per‑event updates harder and
  increases the cost of point reads.
* **Materialised projections** – Maintain additional containers
  containing pre‑aggregated data (e.g. hourly or daily summaries) or
  materialised views optimised for specific queries.  This pattern
  reduces RU consumption on the primary container and improves
  query latency.
* **Hierarchical partition keys** – Combine multiple keys (e.g.
  `tenant_id`, `device_type`, `sensor_id`) to create a hierarchical
  partition key.  This allows co‑locating related sensors while
  still supporting high fan‑out queries using `IN` predicates.

## Capacity Planning and Billing

Cosmos DB charges for both provisioned throughput and storage.  To
plan budgets effectively:

* **Estimate RU per operation** – Reads scale roughly linearly with
  document size; writes cost 5 RU for each 1 KB block.  Use the
  [Request Unit Calculator](https://cosmos.azure.com/capacitycalculator)
  to estimate consumption for your schema.  Summaries are tiny
  compared to raw readings and therefore require fewer RU/s.
* **Adopt hierarchical TTLs** – Set a default `defaultTtl` on the
  container to automatically remove raw readings after a specified
  time (e.g. 30 days).  For individual documents that need longer
  retention, override the TTL value.  Summaries can have longer
  retention to support historical analysis.
* **Monitor storage utilisation** – Use the Azure portal to track
  storage growth and configure alerts.  Cleanup policies (e.g.
  summariser discarding old state) help limit storage costs.
* **Use cost analysis tools** – Azure Cost Management and billing
  exports allow you to attribute RU charges to specific resources or
  tenants.  Tag resources accordingly and review usage trends
  monthly.

By applying these advanced modelling patterns and capacity planning
techniques, you can build robust schemas that evolve with your
business requirements while controlling costs.
